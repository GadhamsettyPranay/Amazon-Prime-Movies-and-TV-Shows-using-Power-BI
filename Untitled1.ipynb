{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAubLlzFssyVGC/uB8eKZW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GadhamsettyPranay/Amazon-Prime-Movies-and-TV-Shows-using-Power-BI/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q73DdUtT1Eyo"
      },
      "outputs": [],
      "source": [
        "#21BDS0295\n",
        "#Module-1\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# 1. Loading the Dataset from the URL\n",
        "url = 'https://raw.githubusercontent.com/salemprakash/EDA/main/Data/txhousing.csv'\n",
        "df = pd.read_csv(url)\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# 2. Summary Statistics for Numerical and Categorical Data\n",
        "print(\"\\nSummary statistics for numerical data:\")\n",
        "print(df.describe())  # Summary for numerical features\n",
        "\n",
        "# Checking for categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "print(\"\\nCategorical Columns:\")\n",
        "print(categorical_cols)\n",
        "\n",
        "# Summary for categorical columns\n",
        "if len(categorical_cols) > 0:\n",
        "    print(\"\\nSummary for categorical columns:\")\n",
        "    print(df[categorical_cols].describe())\n",
        "\n",
        "# 3. Data Cleaning (Handling Missing Values)\n",
        "print(\"\\nMissing values in the dataset:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Dropping rows with missing data\n",
        "df_cleaned = df.dropna()\n",
        "print(\"\\nData after dropping missing values:\")\n",
        "print(df_cleaned.head())\n",
        "\n",
        "# Filling missing data\n",
        "df_filled = df.fillna(method='ffill')\n",
        "print(\"\\nData after forward fill:\")\n",
        "print(df_filled.head())\n",
        "\n",
        "# 4. Visualization (for Numerical and Categorical Data)\n",
        "# Histogram for continuous numerical data (Median price)\n",
        "plt.figure(figsize=(8, 6))\n",
        "df['median'].hist(bins=20)\n",
        "plt.title('Histogram of Median Prices')\n",
        "plt.xlabel('Median Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Bar plot for a categorical variable (if available)\n",
        "if len(categorical_cols) > 0:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    df[categorical_cols[0]].value_counts().plot(kind='bar')\n",
        "    plt.title(f'Bar Chart of {categorical_cols[0]}')\n",
        "    plt.xlabel(categorical_cols[0])\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "# 5. Measurement Scales (Nominal, Ordinal, Interval, Ratio)\n",
        "# For Nominal (e.g., city names):\n",
        "if 'city' in df.columns:\n",
        "    print(\"\\nValue counts for 'city' column (Nominal scale example):\")\n",
        "    print(df['city'].value_counts())\n",
        "\n",
        "# For Ordinal (Creating an ordinal column based on median prices)\n",
        "df['rank'] = pd.qcut(df['median'], 5, labels=['very low', 'low', 'medium', 'high', 'very high'])\n",
        "print(\"\\nFirst 5 rows with 'rank' (Ordinal scale example):\")\n",
        "print(df[['median', 'rank']].head())\n",
        "\n",
        "# 6. Comparing EDA with Classical and Bayesian Analysis\n",
        "# Classical Analysis (T-test)\n",
        "t_stat, p_value = stats.ttest_1samp(df['median'].dropna(), popmean=150000)\n",
        "print(f\"\\nT-test: t-stat = {t_stat}, p-value = {p_value}\")\n",
        "\n",
        "# Bayesian Analysis (PyMC3 or other packages can be used; requires specific setup)\n",
        "\n",
        "# 7. Software Tools for EDA\n",
        "print(\"\\nSoftware Tools for EDA:\")\n",
        "print(\"Python: Pandas, Seaborn, Matplotlib\")\n",
        "print(\"R: GGPlot2, dplyr, tidyr\")\n",
        "print(\"Excel/Tableau: For basic EDA tasks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21BDS0295\n",
        "#MODULE-2\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "# 1. Loading the Dataset\n",
        "url = 'https://raw.githubusercontent.com/salemprakash/EDA/main/Data/txhousing.csv'\n",
        "df = pd.read_csv(url)\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# 2. Data Deduplication\n",
        "print(\"\\nShape before deduplication:\", df.shape)\n",
        "df_deduped = df.drop_duplicates()\n",
        "print(\"Shape after deduplication:\", df_deduped.shape)\n",
        "\n",
        "# 3. Replacing Values\n",
        "# Replace specific values in a column\n",
        "df_replaced = df.copy()\n",
        "df_replaced['sales'] = df_replaced['sales'].replace(0, np.nan)  # Example: Replace '0' in 'sales' with NaN\n",
        "print(\"\\nSales column after replacing 0 with NaN:\")\n",
        "print(df_replaced[['sales']].head())\n",
        "\n",
        "# Replace NaN values with a specific value\n",
        "df_replaced.fillna(value={'sales': df_replaced['sales'].mean()}, inplace=True)\n",
        "print(\"\\nSales column after replacing NaN with mean value:\")\n",
        "print(df_replaced[['sales']].head())\n",
        "\n",
        "# 4. Discretization and Binning\n",
        "# Bin 'median' column into categories\n",
        "df_binned = df.copy()\n",
        "df_binned['median_binned'] = pd.cut(df_binned['median'].fillna(df_binned['median'].mean()), bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']) # Fill NaN values with the mean of the column before binning\n",
        "print(\"\\nBinned 'median' column:\")\n",
        "print(df_binned[['median', 'median_binned']].head())\n",
        "\n",
        "# 5. Handling Missing Data - Traditional Method (Maximum Likelihood Estimation)\n",
        "# Introduce some missing values for demonstration\n",
        "df_with_na = df.copy()\n",
        "df_with_na.loc[10:15, 'sales'] = np.nan  # Creating missing values in 'sales'\n",
        "\n",
        "# Handle missing data using SimpleImputer (Maximum Likelihood Estimation approximated via mean substitution here)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df_with_na['sales'] = imputer.fit_transform(df_with_na[['sales']])\n",
        "print(\"\\nSales column after imputing missing values using MLE approximation (mean):\")\n",
        "print(df_with_na[['sales']].head(20))\n",
        "\n",
        "# 6. Discretization with KBinsDiscretizer\n",
        "kbins = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='uniform')\n",
        "# Impute missing values with the mean of the column before discretization\n",
        "df['median_binned_kbins'] = kbins.fit_transform(df[['median']].fillna(df['median'].mean()))\n",
        "print(\"\\nDiscretized 'median' column using KBinsDiscretizer:\")\n",
        "print(df[['median', 'median_binned_kbins']].head())\n",
        "\n",
        "# 7. Visualizing Binned Data\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Access the 'median_binned' column from the correct DataFrame (df_binned)\n",
        "df_binned['median_binned'].value_counts().plot(kind='bar')\n",
        "plt.title('Distribution of Median Price Bins')\n",
        "plt.xlabel('Median Price Bins')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vywnly9X8yvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21BDS0295\n",
        "#MODULE-3\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "url = 'https://raw.githubusercontent.com/salemprakash/EDA/main/Data/txhousing.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# 1. Univariate Analysis\n",
        "print(\"Univariate Analysis (Summary Statistics):\")\n",
        "print(df.describe())\n",
        "\n",
        "# Visualizing Univariate Data (Histograms for Numerical Features)\n",
        "plt.figure(figsize=(10, 6))\n",
        "df['median'].hist(bins=20)\n",
        "plt.title('Histogram of Median Prices')\n",
        "plt.xlabel('Median Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# 2. Bivariate Analysis\n",
        "print(\"\\nBivariate Analysis (Correlation Matrix):\")\n",
        "# Select only numerical columns for correlation calculation\n",
        "correlation_matrix = df.select_dtypes(include=np.number).corr()\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Heatmap of Correlation Matrix\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix Heatmap')\n",
        "plt.show()\n",
        "\n",
        "# Scatter Plot for Bivariate Analysis\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x='median', y='sales', data=df)\n",
        "plt.title('Scatter Plot of Median Prices vs Sales')\n",
        "plt.xlabel('Median Price')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()\n",
        "\n",
        "# 3. Multivariate Analysis\n",
        "print(\"\\nMultivariate Analysis (Pairplot):\")\n",
        "sns.pairplot(df[['median', 'sales', 'inventory', 'volume']])\n",
        "plt.show()\n",
        "\n",
        "# 4. Time Series Analysis (TSA)\n",
        "# Convert 'date' column to datetime\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Time-based indexing\n",
        "df.set_index('date', inplace=True)\n",
        "print(\"\\nDataset with time-based index:\")\n",
        "print(df.head())\n",
        "\n",
        "# Visualizing Time Series Data\n",
        "plt.figure(figsize=(10, 6))\n",
        "df['sales'].plot()\n",
        "plt.title('Sales Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()\n",
        "\n",
        "# Grouping Time Series Data (Yearly)\n",
        "df['year'] = df.index.year\n",
        "yearly_sales = df.groupby('year')['sales'].sum()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "yearly_sales.plot(kind='bar')\n",
        "plt.title('Yearly Sales')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.show()\n",
        "\n",
        "# Resampling Time Series Data (Monthly)\n",
        "monthly_sales = df['sales'].resample('M').sum()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "monthly_sales.plot()\n",
        "plt.title('Monthly Sales Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Monthly Sales')\n",
        "plt.show()\n",
        "\n",
        "# Resampling for Quarterly Data\n",
        "quarterly_sales = df['sales'].resample('Q').sum()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "quarterly_sales.plot(kind='bar')\n",
        "plt.title('Quarterly Sales Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Quarterly Sales')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BlIj-Jeo_NRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21BDS0295\n",
        "#MODULE-4\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Load the Dataset\n",
        "url = 'https://raw.githubusercontent.com/salemprakash/EDA/main/Data/txhousing.csv'\n",
        "df = pd.read_csv(url)\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# 2. Statistical Summary Measures\n",
        "print(\"\\nStatistical Summary:\")\n",
        "print(df.describe())  # Numerical columns summary\n",
        "\n",
        "# 3. Data Elaboration (Show basic info and null values)\n",
        "print(\"\\nData Information:\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# 4. 1-D Statistical Data Analysis (Univariate Analysis)\n",
        "# Histogram for a numerical column (e.g., 'median')\n",
        "plt.figure(figsize=(8, 6))\n",
        "df['median'].plot(kind='hist', bins=20)\n",
        "plt.title('1-D Statistical Analysis: Histogram of Median Prices')\n",
        "plt.xlabel('Median Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# 5. 2-D Statistical Data Analysis (Bivariate Analysis)\n",
        "# Scatter plot between 'median' and 'sales'\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['median'], df['sales'], alpha=0.6)\n",
        "plt.title('2-D Statistical Analysis: Scatter Plot (Median vs Sales)')\n",
        "plt.xlabel('Median Price')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()\n",
        "\n",
        "# 6. n-D Statistical Data Analysis (Multivariate Analysis)\n",
        "# Pairplot (for 3 variables: 'median', 'sales', 'volume')\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.pairplot(df[['median', 'sales', 'volume']].dropna())\n",
        "plt.suptitle('n-D Statistical Data Analysis: Pairplot (Median, Sales, Volume)', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# 7. Contingency Table\n",
        "# Creating a categorical column from 'median' and cross-tabulating it with 'sales'\n",
        "df['median_category'] = pd.qcut(df['median'], 4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
        "contingency_table = pd.crosstab(df['median_category'], df['sales'].apply(lambda x: 'Low' if x < 500 else 'High'))\n",
        "print(\"\\nContingency Table (Median Category vs Sales):\")\n",
        "print(contingency_table)\n",
        "\n",
        "# 8. Visualization (Scatter plots, Dot charts, Bar Plots)\n",
        "# Scatter Plot (Median vs Volume)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['median'], df['volume'], alpha=0.6, color='g')\n",
        "plt.title('Scatter Plot: Median vs Volume')\n",
        "plt.xlabel('Median Price')\n",
        "plt.ylabel('Volume')\n",
        "plt.show()\n",
        "\n",
        "# Dot Chart (Dot plot for 'sales')\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(df['sales'], 'o', color='b')\n",
        "plt.title('Dot Chart: Sales')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()\n",
        "\n",
        "# Bar Plot (Bar chart for median category counts)\n",
        "plt.figure(figsize=(8, 6))\n",
        "df['median_category'].value_counts().plot(kind='bar', color='orange')\n",
        "plt.title('Bar Plot: Median Category Counts')\n",
        "plt.xlabel('Median Category')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YaS4pN2B_h3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21BDS0295\n",
        "#MODULE-5\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import SpectralClustering, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.datasets import make_blobs\n",
        "from scipy.sparse.csgraph import minimum_spanning_tree\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Load the dataset from the provided link\n",
        "url = \"https://raw.githubusercontent.com/salemprakash/EDA/main/Data/txhousing.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Example clustering on the 'median' and 'year' columns\n",
        "X = df[['median', 'year']].dropna()\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# 1. Spectral Clustering\n",
        "spectral_clustering = SpectralClustering(n_clusters=3, affinity='nearest_neighbors')\n",
        "labels_spectral = spectral_clustering.fit_predict(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_spectral, cmap='viridis')\n",
        "plt.title('Spectral Clustering')\n",
        "plt.xlabel('Scaled Median')\n",
        "plt.ylabel('Year')\n",
        "plt.show()\n",
        "\n",
        "# 2. Document Clustering (Example with synthetic data)\n",
        "# Generating synthetic data for document clustering\n",
        "documents, _ = make_blobs(n_samples=300, centers=5, cluster_std=0.60, random_state=0)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(documents[:, 0], documents[:, 1], s=30)\n",
        "plt.title('Synthetic Data for Document Clustering')\n",
        "plt.show()\n",
        "\n",
        "# Using KMeans for document clustering\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=5)\n",
        "labels_kmeans = kmeans.fit_predict(documents)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(documents[:, 0], documents[:, 1], c=labels_kmeans, cmap='viridis')\n",
        "plt.title('Document Clustering using KMeans')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n",
        "\n",
        "# 3. Minimum Spanning Tree Clustering\n",
        "mst = minimum_spanning_tree(np.corrcoef(X_scaled.T))\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.spy(mst, markersize=5)\n",
        "plt.title('Minimum Spanning Tree')\n",
        "plt.show()\n",
        "\n",
        "# 4. Model-based Clustering - Expectation-Maximization Algorithm\n",
        "gmm = GaussianMixture(n_components=3)\n",
        "gmm.fit(X_scaled)\n",
        "labels_gmm = gmm.predict(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_gmm, cmap='viridis')\n",
        "plt.title('Model-based Clustering using EM Algorithm')\n",
        "plt.xlabel('Scaled Median')\n",
        "plt.ylabel('Year')\n",
        "plt.show()\n",
        "\n",
        "# 5. Hierarchical Agglomerative Clustering\n",
        "agglo = AgglomerativeClustering(n_clusters=3)\n",
        "labels_agglo = agglo.fit_predict(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_agglo, cmap='viridis')\n",
        "plt.title('Hierarchical Agglomerative Clustering')\n",
        "plt.xlabel('Scaled Median')\n",
        "plt.ylabel('Year')\n",
        "plt.show()\n",
        "\n",
        "# 6. Outlier Detection using Clustering\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "iso_forest = IsolationForest(contamination=0.1)\n",
        "outliers = iso_forest.fit_predict(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=outliers, cmap='coolwarm')\n",
        "plt.title('Outlier Detection using Isolation Forest')\n",
        "plt.xlabel('Scaled Median')\n",
        "plt.ylabel('Year')\n",
        "plt.show()\n",
        "\n",
        "# Silhouette Scores for evaluating clustering performance\n",
        "print(\"Silhouette Score for Spectral Clustering:\", silhouette_score(X_scaled, labels_spectral))\n",
        "print(\"Silhouette Score for KMeans Clustering:\", silhouette_score(documents, labels_kmeans))\n",
        "print(\"Silhouette Score for GMM Clustering:\", silhouette_score(X_scaled, labels_gmm))\n",
        "print(\"Silhouette Score for Agglomerative Clustering:\", silhouette_score(X_scaled, labels_agglo))\n"
      ],
      "metadata": {
        "id": "r-4IsESu_rNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21BDS0295\n",
        "#MODULE-6\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE, MDS\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the dataset\n",
        "url = 'https://raw.githubusercontent.com/salemprakash/EDA/main/Data/txhousing.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Select numerical columns for dimensionality reduction and clustering\n",
        "numerical_cols = df.select_dtypes(include=np.number).columns\n",
        "df_numerical = df[numerical_cols]\n",
        "\n",
        "# 1. Standardize the data\n",
        "scaler = StandardScaler()\n",
        "# Impute NaNs with the mean of each column before scaling\n",
        "df_scaled = scaler.fit_transform(df_numerical.fillna(df_numerical.mean()))\n",
        "\n",
        "# 2. Principal Component Analysis (PCA)\n",
        "pca = PCA(n_components=2)  # Reduce to 2 principal components\n",
        "principal_components = pca.fit_transform(df_scaled)\n",
        "df_pca = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
        "\n",
        "# 3. t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "tsne_results = tsne.fit_transform(df_scaled)\n",
        "df_tsne = pd.DataFrame(data=tsne_results, columns=['t-SNE1', 't-SNE2'])\n",
        "\n",
        "# 4. Multidimensional Scaling (MDS)\n",
        "mds = MDS(n_components=2, random_state=42)\n",
        "mds_results = mds.fit_transform(df_scaled)\n",
        "df_mds = pd.DataFrame(data=mds_results, columns=['MDS1', 'MDS2'])\n",
        "\n",
        "# 5. K-Means Clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)  # Example with 3 clusters\n",
        "clusters = kmeans.fit_predict(df_scaled)\n",
        "df_numerical['cluster'] = clusters\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# PCA plot\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(df_pca['PC1'], df_pca['PC2'])\n",
        "plt.title('PCA')\n",
        "\n",
        "# t-SNE plot\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(df_tsne['t-SNE1'], df_tsne['t-SNE2'])\n",
        "plt.title('t-SNE')\n",
        "\n",
        "# MDS plot\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(df_mds['MDS1'], df_mds['MDS2'])\n",
        "plt.title('MDS')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize clusters (using PCA for example)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df_pca['PC1'], df_pca['PC2'], c=df_numerical['cluster'])\n",
        "plt.title('Clusters visualized with PCA')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hfw8b5mfAA-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21BDS0295\n",
        "#MODULE-7\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the dataset from the provided link\n",
        "url = \"https://raw.githubusercontent.com/salemprakash/EDA/main/Data/txhousing.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Data Cleaning (Handle missing values if any)\n",
        "df.dropna(inplace=True)  # Drop rows with missing values\n",
        "\n",
        "# 1. Constructing Linear Regression Model\n",
        "# Assuming 'median' is the target variable and other columns are features\n",
        "# Selecting numerical columns for the model\n",
        "X = df.select_dtypes(include=[np.number]).drop(columns=['median'])\n",
        "y = df['median']\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 2. Making Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 3. Evaluating the Model\n",
        "# Computing Mean Squared Error and R-squared value\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nMean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")\n",
        "\n",
        "# Plotting Actual vs Predicted values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.6)\n",
        "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')  # 45-degree line\n",
        "plt.xlabel('Actual Median Prices')\n",
        "plt.ylabel('Predicted Median Prices')\n",
        "plt.title('Actual vs Predicted Median Prices')\n",
        "plt.show()\n",
        "\n",
        "# Understanding Accuracy\n",
        "# Since this is regression, accuracy is not computed like in classification tasks\n",
        "print(\"\\nIn regression tasks, accuracy is not defined in the same way. We typically use metrics like MSE, R-squared, etc., to evaluate model performance.\")\n",
        "\n",
        "# 4. Understanding Reinforcement Learning\n",
        "print(\"\\nReinforcement Learning (RL):\")\n",
        "print(\"Reinforcement Learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward.\")\n",
        "print(\"\\nDifference between Supervised Learning and Reinforcement Learning:\")\n",
        "print(\"1. Supervised Learning: The model learns from labeled data (input-output pairs).\")\n",
        "print(\"2. Reinforcement Learning: The model learns from the consequences of its actions (rewards or penalties).\")\n",
        "\n",
        "# Applications of Reinforcement Learning\n",
        "print(\"\\nApplications of Reinforcement Learning:\")\n",
        "print(\"1. Robotics: For teaching robots to perform tasks through trial and error.\")\n",
        "print(\"2. Game Playing: For training agents to play games like Chess or Go.\")\n",
        "print(\"3. Autonomous Vehicles: For navigation and decision-making.\")\n",
        "print(\"4. Personalized Recommendations: For dynamically adjusting content based on user interactions.\")\n"
      ],
      "metadata": {
        "id": "wVkAmmbPFWIc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}